{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPRQDbGg1eEzQufAsRnXU0u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import numpy as np \n","import pandas as pd \n","import os\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, GlobalAveragePooling2D, Dropout, Flatten\n","from tensorflow.keras.applications import VGG16\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"metadata":{"id":"REA-SLwaCUw8","executionInfo":{"status":"ok","timestamp":1684553682526,"user_tz":-540,"elapsed":5080,"user":{"displayName":"신성한","userId":"02352508022902564974"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["train_ds=\"/kaggle/input/gender-classification-dataset/Training\"\n","test_ds=\"/kaggle/input/gender-classification-dataset/Validation\""],"metadata":{"id":"wX6p6J3tFLxZ","executionInfo":{"status":"ok","timestamp":1684553692022,"user_tz":-540,"elapsed":476,"user":{"displayName":"신성한","userId":"02352508022902564974"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["root_path = \"/kaggle/input/gender-classification-dataset/Training/\"\n","class_names = sorted(os.listdir(root_path))\n","n_classes = len(class_names)\n","class_dis = [len(os.listdir(root_path + name)) for name in class_names]\n","print(f\"Total Number of Classes : {n_classes} \\nClass Names : {class_names}\")\n","print(f\"female : {class_dis[0]} \\nmale : {class_dis[1]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"lh06MrJ0FdpK","executionInfo":{"status":"error","timestamp":1684553696769,"user_tz":-540,"elapsed":556,"user":{"displayName":"신성한","userId":"02352508022902564974"}},"outputId":"3b472853-982c-466a-a301-fe40aae4ecb7"},"execution_count":3,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-cd658a0dff0f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroot_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/gender-classification-dataset/Training/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclass_dis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total Number of Classes : {n_classes} \\nClass Names : {class_names}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/gender-classification-dataset/Training/'"]}]},{"cell_type":"code","source":["os.listdir('/kaggle/input/gender-classification-dataset/Training')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wu8NCv8-IveX","executionInfo":{"status":"ok","timestamp":1684552972807,"user_tz":-540,"elapsed":4,"user":{"displayName":"신성한","userId":"02352508022902564974"}},"outputId":"97faa7c9-ba1e-4587-d866-94494c97616f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AI/AI_Project/FaceRecognition/dataset/0_train\n","/content/drive/MyDrive/AI/AI_Project/FaceRecognition/dataset/1_test\n","/content/drive/MyDrive/AI/AI_Project/FaceRecognition/dataset/0_train\n","/content/drive/MyDrive/AI/AI_Project/FaceRecognition/dataset/1_test\n"]}]},{"cell_type":"code","source":["import plotly.express as px"],"metadata":{"id":"J9qVmUjHJDYn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = px.pie(names=class_names, values=class_dis,\n","             title=\"Training Class Distribution\", hole=0.4)\n","fig.update_layout({'title':{'x':0.48}})\n","fig.show()"],"metadata":{"id":"1mKSA6FsCVPH","colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"status":"error","timestamp":1684552980687,"user_tz":-540,"elapsed":1148,"user":{"displayName":"신성한","userId":"02352508022902564974"}},"outputId":"94d35ebc-6ff9-4d88-f46e-d43dd49873d7"},"execution_count":null,"outputs":[{"output_type":"error","ename":"UnidentifiedImageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-3697e5bf5c85>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_file_0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimg_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"0_train/\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mimg_arr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3028\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccept_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m     raise UnidentifiedImageError(\n\u001b[0m\u001b[1;32m   3031\u001b[0m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m     )\n","\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7f5c0b3bdc60>"]}]},{"cell_type":"code","source":["\n","valid_path = \"/kaggle/input/gender-classification-dataset/Validation/\"\n","valid_dis = [len(os.listdir(valid_path + name)) for name in class_names]\n","fig = px.pie(names=class_names, values=valid_dis,\n","             title=\"test Class Distribution\", hole=0.4)\n","fig.update_layout({'title':{'x':0.48}})\n","fig.show()"],"metadata":{"id":"LKjpsfi0CVRu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_datagen = ImageDataGenerator(rescale = 1./255)#initialize train generator \n","                                 \n","\n","\n","valid_datagen = ImageDataGenerator(rescale = 1.0/255.) #initialize validation generator \n","\n","test_datagen = ImageDataGenerator(rescale = 1.0/255.) #initialize test generator"],"metadata":{"id":"R4VPONLeCVVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_datagen = ImageDataGenerator(zoom_range=0.15,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15)\n","\n","test_datagen = ImageDataGenerator()\n","\n","\n","train_generator = train_datagen.flow_from_directory(train_ds,target_size=(224, 224),batch_size=32,shuffle=True,class_mode='categorical')\n","\n","test_generator = test_datagen.flow_from_directory(test_ds,target_size=(224,224),batch_size=32,shuffle=False,class_mode='categorical')"],"metadata":{"id":"JH_KHeRhCVXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_generator[0][0].shape"],"metadata":{"id":"_97NEeAkCVZh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img= train_generator[0]\n","print(img)"],"metadata":{"id":"OeG0v7iyCVbp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(train_generator)"],"metadata":{"id":"ow5iKIJACVeH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img = train_generator[0]\n","print(img[0].shape) # shape of the input batch\n","print(img[1].shape) # shape of the target labels"],"metadata":{"id":"3hndPmP6CiX6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(train_generator))\n","print(len(test_ds))"],"metadata":{"id":"FaLnsNkZCiik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from skimage import io\n","\n","# Load image\n","img_url = \"/kaggle/input/gender-classification-dataset/Training/male/090646.jpg.jpg\"\n","img = io.imread(img_url)\n","\n","# Display image\n","plt.imshow(img)\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"mzwTdT8uCioi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Softmax\n","from tensorflow.keras.preprocessing import image\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"metadata":{"id":"dN-V16JjCiwF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build the Sequential feedforward neural network model\n","\n","seq_model = Sequential([ \n","    Flatten(input_shape=(224,224,3), name='input_layer'),\n","      Dense(128, activation='relu', name='layer1'),\n","    Dense(64, activation='relu', name='layer2'),\n","    # Dense(64, activation='relu', name='layer2'),\n","    Dense(32, activation='relu', name='layer3'),\n","    # Dense(32, activation='relu', name='layer4'),\n","    Dense(2, activation='softmax', name='output_layer')\n","])\n","       "],"metadata":{"id":"oRsFidnWCi2m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the model summary\n","\n","seq_model.summary()"],"metadata":{"id":"fOLperybCVgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense"],"metadata":{"id":"HfIvECSxCvqG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.optimizers import Adam\n","opt = Adam(learning_rate=0.00001)\n","seq_model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"],"metadata":{"id":"9vyCaXsTCyaJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history =seq_model.fit(train_generator, validation_data=test_generator, epochs=10)"],"metadata":{"id":"-fRha7cgCz7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_categories = len(os.listdir('/kaggle/input/gender-classification-dataset/Validation'))# number of categories print(n_categories)"],"metadata":{"id":"f3VjfVHUC2o_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_categories"],"metadata":{"id":"bS-NcqUPC35t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results =pd.DataFrame(history.history)\n","results.head()"],"metadata":{"id":"154XyrwnC6UJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(15,6))\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model Loss')\n","plt.ylabel('loss')\n","plt.xlabel('epochs')\n","plt.legend(['Train','Val'], loc= 'upper left')\n","plt.show()"],"metadata":{"id":"DZPIUVXqOHJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(15,5))\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model Accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['Train', 'Val'], loc='upper left')\n","plt.show()"],"metadata":{"id":"E-iYGT0QOIeg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test_1 = test_generator.classes\n","y_pred_1 =seq_model.predict(test_generator)\n","y_pred_1 = np.argmax(y_pred_1,axis=1)"],"metadata":{"id":"Hn1DrexROLDG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results =seq_model.evaluate(test_generator)"],"metadata":{"id":"gtLABxwxOMu6"},"execution_count":null,"outputs":[]}]}